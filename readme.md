# ChatKit Ã— Ollama (Local AI Chat)

A minimal **ChatKit UI + FastAPI backend** setup that connects to a **local Ollama model**.  
This lets you run a ChatGPT-style interface entirely on your machine â€” no OpenAI API costs.

---

## ðŸ§  Overview

- **Frontend:** Next.js + ChatKit (React)
- **Backend:** FastAPI (Python)
- **Model Runtime:** Ollama (`http://localhost:11434/v1`)
- **Cost:** ðŸ’¸ Free â€” all inference is local

You can later swap the model (e.g., `llama3.1`, `qwen2.5`, etc.) or add cloud fallbacks.

---

ðŸ”§ Key Features

ðŸ”¹ ChatKit UI: Prebuilt chat interface for modern apps

ðŸ”¹ FastAPI Backend: Lightweight, async, and easy to extend

ðŸ”¹ OpenAI-Compatible Schema: Works with any compliant model

ðŸ”¹ Configurable Environment: Switch endpoints via .env

ðŸ”¹ Modular Extensions: Memory, streaming, and tool calls



